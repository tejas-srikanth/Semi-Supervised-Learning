{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0071e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tejas_1n\\anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Core PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Data loading and augmentation\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Clustering (for offline K-means)\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Optional utilities\n",
    "import random, copy, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43975951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170499072it [00:03, 46614616.02it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                download=True, transform=transform)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38476baa",
   "metadata": {},
   "source": [
    "### Create a class to make a Semi Supervised dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c7f7a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemiSupervisedDataset(Dataset):\n",
    "    def __init__(self, dataset, M):\n",
    "        self.dataset = dataset\n",
    "        self.M = M\n",
    "        self.indices = list(range(len(dataset)))\n",
    "        random.shuffle(self.indices)  # Shuffle indices for randomness\n",
    "        self.labeled_indices = self.indices[M:]\n",
    "        self.unlabeled_indices = self.indices[:M]\n",
    "        self.unlabeled_dset = self.dataset[self.unlabeled_indices]\n",
    "        self.labeled_dset = self.dataset[self.labeled_indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labeled_dset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < self.labeled_dset.__len__():\n",
    "            x, y = self.labeled_dset[idx]\n",
    "            return x, y\n",
    "        else:\n",
    "            raise IndexError(\"Index out of range\")\n",
    "\n",
    "def semi_supervised_batch_loader(dataset, batch_size, M):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader that returns batches of (labeled_x, labeled_y, unlabeled_x).\n",
    "    Args:\n",
    "        dataset: The original dataset (e.g., trainset).\n",
    "        batch_size: The size of the batch.\n",
    "        M: Number of labeled instances.\n",
    "    \"\"\"\n",
    "    semi_supervised_dataset = SemiSupervisedDataset(dataset, M)\n",
    "    labeled_x, labeled_y, unlabeled_x = [], [], semi_supervised_dataset.unlabeled_dset\n",
    "\n",
    "    for idx in range(len(semi_supervised_dataset.labeled_dset)):\n",
    "        x, y = semi_supervised_dataset[idx]\n",
    "        labeled_x.append(x)\n",
    "        labeled_y.append(y)\n",
    "\n",
    "        # Yield a batch when the batch size is reached\n",
    "        if len(labeled_x) == batch_size:\n",
    "            yield labeled_x, labeled_y, unlabeled_x\n",
    "            labeled_x, labeled_y, unlabeled_x = [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e924711b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainset_semisupervised \u001b[38;5;241m=\u001b[39m \u001b[43mSemiSupervisedDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m trainloader \u001b[38;5;241m=\u001b[39m semi_supervised_batch_loader(trainset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, M\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "Cell \u001b[1;32mIn[23], line 9\u001b[0m, in \u001b[0;36mSemiSupervisedDataset.__init__\u001b[1;34m(self, dataset, M)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabeled_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[M:]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munlabeled_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[:M]\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munlabeled_dset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlabeled_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabeled_dset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabeled_indices]\n",
      "File \u001b[1;32mc:\\Users\\tejas_1n\\anaconda3\\envs\\myenv\\lib\\site-packages\\torchvision\\datasets\\cifar.py:113\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, Any]:\n\u001b[0;32m    106\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m        index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m        tuple: (image, target) where target is index of the target class.\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "trainset_semisupervised = SemiSupervisedDataset(trainset, M=1000)\n",
    "trainloader = semi_supervised_batch_loader(trainset, batch_size=64, M=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cbfa88",
   "metadata": {},
   "source": [
    "\n",
    "### Create a basic CNN model for dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c47f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x, return_logits=False):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x_logits = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x_logits)\n",
    "        if return_logits:\n",
    "            return x, x_logits\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29da1d96",
   "metadata": {},
   "source": [
    "### Create Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46ccd894",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalConnected(nn.Module):\n",
    "    def __init__(self, input_layers, num_classes=10):\n",
    "        super(FinalConnected, self).__init__()\n",
    "        self.fc = nn.Linear(input_layers, input_layers // 2)\n",
    "        self.fc2 = nn.Linear(input_layers // 2, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47f1155",
   "metadata": {},
   "source": [
    "### Train on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e9168e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()\n",
    "optim = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e870e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(size=32, padding=4),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(3)], p=0.2),\n",
    "])\n",
    "\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(size=32, padding=4),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cea34ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 64\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m labeled_x, labeled_y, unlabeled_x \u001b[38;5;129;01min\u001b[39;00m trainloader:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(labeled_x), \u001b[38;5;28mlen\u001b[39m(labeled_y), \u001b[38;5;28mlen\u001b[39m(unlabeled_x))\n\u001b[1;32m---> 10\u001b[0m     labeled_x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabeled_x\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m     labeled_y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(labeled_y)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m     unlabeled_x_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(unlabeled_x)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "device = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'gpu':\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "\n",
    "for epoch in range(10):  \n",
    "    for labeled_x, labeled_y, unlabeled_x in trainloader:\n",
    "        print(len(labeled_x), len(labeled_y), len(unlabeled_x))\n",
    "        labeled_x = torch.stack(labeled_x).to(device)\n",
    "        labeled_y = torch.tensor(labeled_y).to(device)\n",
    "        unlabeled_x_batch = torch.stack(unlabeled_x).to(device)\n",
    "\n",
    "        # ----- Feature and output pass -----\n",
    "        z_labeled, logits_labeled = model(labeled_x, return_logits=True)\n",
    "        z_unlabeled, logits_unlabeled = model(unlabeled_x_batch, return_logits=True)\n",
    "\n",
    "        # ----- Loss 1: Supervised -----\n",
    "        L_sup = criterion(logits_labeled, labeled_y)\n",
    "\n",
    "        # ----- Loss 2: K-means clustering on unlabeled features -----\n",
    "        with torch.no_grad():\n",
    "            z_unlabeled_np = z_unlabeled.cpu().numpy()\n",
    "            kmeans = KMeans(n_clusters=10, n_init=10).fit(z_unlabeled_np)\n",
    "            pseudo_labels = torch.tensor(kmeans.labels_).to(torch.long).to(device)\n",
    "            cluster_centers = torch.tensor(kmeans.cluster_centers_).to(device)\n",
    "        L_kmeans = torch.mean((z_unlabeled - cluster_centers[pseudo_labels]) ** 2) \n",
    "\n",
    "        # ----- Loss 3: Consistency -----\n",
    "        x_weak, x_strong = weak_transform(unlabeled_x_batch), strong_transform(unlabeled_x_batch)\n",
    "        z_weak, logits_weak = model(x_weak, return_logits=True)\n",
    "        z_strong, logits_strong = model(x_strong, return_logits=True)\n",
    "        L_consistency = F.kl_div(F.log_softmax(logits_weak, dim=1), F.softmax(logits_strong, dim=1), reduction='batchmean')\n",
    "\n",
    "        # ----- Combine -----\n",
    "        loss = L_sup + 0.25 * L_kmeans + 0.25 * L_consistency\n",
    "\n",
    "        # ----- Backpropagation -----\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/10], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f2a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
